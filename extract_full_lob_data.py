#!/usr/bin/env python3
"""
Enhanced LOB Data Extractor for Attention-Based Forecasting

This script extracts the full 40+ days of LOB data from the database with intelligent
gap handling and session-aware processing. It replaces the overly-restrictive clean
data filtering with a more sophisticated approach.

Key Features:
1. Processes 40+ days of high-quality data (instead of 1 day)
2. Intelligent gap handling (forward fill, interpolation, session splitting)
3. Session-aware processing (maintains continuity)
4. Memory-efficient streaming from database
5. Structured output for model training
"""

import os
import numpy as np
import pandas as pd
import psycopg2
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Tuple, Optional
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Database Configuration
DB_CONFIG = {
    'host': 'localhost',
    'port': 5433,
    'user': 'backtest_user',
    'password': 'backtest_password',
    'database': 'backtest_db'
}

# Configuration
OUTPUT_DIR = "data/full_lob_data"
LOB_LEVELS = 5
BATCH_SIZE = 1000000  # Process 1M records at a time
MAX_GAP_MINUTES = 30  # Maximum gap to interpolate
MIN_SESSION_HOURS = 2  # Minimum session length in hours
TARGET_EXCHANGES = ['binance_spot', 'binance_perp', 'bybit_spot']
TARGET_PAIRS = ['BTC-USDT', 'ETH-USDT', 'SOL-USDT', 'WLD-USDT']

# High-quality days (>3M records/day) based on our analysis
HIGH_QUALITY_DAYS = [
    # May 2025
    '2025-05-19', '2025-05-20', '2025-05-21', '2025-05-22', '2025-05-23',
    '2025-05-24', '2025-05-25', '2025-05-26', '2025-05-27', '2025-05-28',
    '2025-05-29', '2025-05-30', '2025-05-31',
    # June 2025 (first half)
    '2025-06-01', '2025-06-02', '2025-06-03', '2025-06-05', '2025-06-06',
    '2025-06-07', '2025-06-08', '2025-06-09', '2025-06-10', '2025-06-11',
    '2025-06-12', '2025-06-13', '2025-06-14', '2025-06-15', '2025-06-16',
    # June 2025 (second half)
    '2025-06-18', '2025-06-19', '2025-06-20', '2025-06-21', '2025-06-22',
    '2025-06-23', '2025-06-24', '2025-06-25', '2025-06-26', '2025-06-27',
    '2025-06-28'
]

# Additional good days (1-3M records/day) - can be added if needed
GOOD_DAYS = [
    '2025-05-18', '2025-06-04', '2025-06-17', '2025-06-29', '2025-06-30',
    '2025-07-01', '2025-07-02', '2025-07-03', '2025-07-04'
]

def get_db_connection():
    """Create database connection."""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        return None

def extract_lob_data(data_dict: dict, levels: int = 5) -> dict:
    """Extract LOB data from JSONB into flat structure."""
    try:
        result = {}
        
        # Extract bids (sorted by price descending)
        if 'bids' in data_dict and isinstance(data_dict['bids'], list):
            bids = sorted(data_dict['bids'], key=lambda x: float(x[0]), reverse=True)[:levels]
            for i, (price, quantity) in enumerate(bids):
                result[f'bid_price_{i+1}'] = float(price)
                result[f'bid_quantity_{i+1}'] = float(quantity)
        
        # Extract asks (sorted by price ascending)
        if 'asks' in data_dict and isinstance(data_dict['asks'], list):
            asks = sorted(data_dict['asks'], key=lambda x: float(x[0]))[:levels]
            for i, (price, quantity) in enumerate(asks):
                result[f'ask_price_{i+1}'] = float(price)
                result[f'ask_quantity_{i+1}'] = float(quantity)
        
        # Fill missing levels with None
        for i in range(1, levels + 1):
            for side in ['bid', 'ask']:
                for field in ['price', 'quantity']:
                    key = f'{side}_{field}_{i}'
                    if key not in result:
                        result[key] = None
        
        return result
        
    except Exception as e:
        logger.warning(f"Error parsing LOB data: {e}")
        return {}

def detect_gaps(df: pd.DataFrame, max_gap_minutes: int = 30) -> List[Tuple[datetime, datetime]]:
    """Detect gaps in time series data."""
    gaps = []
    
    if len(df) < 2:
        return gaps
    
    # Calculate time differences
    time_diffs = df['timestamp'].diff()
    
    # Find gaps larger than threshold
    gap_threshold = timedelta(minutes=max_gap_minutes)
    large_gaps = time_diffs[time_diffs > gap_threshold]
    
    for idx in large_gaps.index:
        gap_start = df.loc[idx-1, 'timestamp']
        gap_end = df.loc[idx, 'timestamp']
        gaps.append((gap_start, gap_end))
    
    return gaps

def split_into_sessions(df: pd.DataFrame, gaps: List[Tuple[datetime, datetime]], 
                       min_session_hours: int = 2) -> List[pd.DataFrame]:
    """Split data into continuous sessions based on gaps."""
    if not gaps:
        return [df] if len(df) > 0 else []
    
    sessions = []
    start_idx = 0
    
    for gap_start, gap_end in gaps:
        # Find the end of current session
        session_end_idx = df[df['timestamp'] <= gap_start].index[-1]
        
        # Create session
        session_df = df.iloc[start_idx:session_end_idx + 1].copy()
        
        # Check if session is long enough
        if len(session_df) > 0:
            duration = session_df['timestamp'].iloc[-1] - session_df['timestamp'].iloc[0]
            if duration >= timedelta(hours=min_session_hours):
                sessions.append(session_df)
        
        # Start next session after gap
        start_idx = df[df['timestamp'] >= gap_end].index[0] if len(df[df['timestamp'] >= gap_end]) > 0 else len(df)
    
    # Add final session
    if start_idx < len(df):
        final_session = df.iloc[start_idx:].copy()
        if len(final_session) > 0:
            duration = final_session['timestamp'].iloc[-1] - final_session['timestamp'].iloc[0]
            if duration >= timedelta(hours=min_session_hours):
                sessions.append(final_session)
    
    return sessions

def process_single_day(date_str: str) -> bool:
    """Process a single day of data."""
    logger.info(f"Processing {date_str}...")
    
    try:
        conn = get_db_connection()
        if not conn:
            return False
        
        # Create output directory for this date
        date_dir = os.path.join(OUTPUT_DIR, date_str)
        os.makedirs(date_dir, exist_ok=True)
        
        # Query data for this date
        query = """
        SELECT timestamp, exchange, trading_pair, data
        FROM lob_snapshots 
        WHERE DATE(timestamp) = %s
        ORDER BY timestamp, exchange, trading_pair
        """
        
        logger.info(f"  Querying database for {date_str}...")
        
        # Use chunked reading for memory efficiency
        chunk_size = BATCH_SIZE
        processed_records = 0
        
        for chunk in pd.read_sql_query(query, conn, params=[date_str], chunksize=chunk_size):
            logger.info(f"  Processing chunk: {len(chunk)} records")
            
            # Process by exchange/pair
            for exchange in TARGET_EXCHANGES:
                for pair in TARGET_PAIRS:
                    pair_data = chunk[
                        (chunk['exchange'] == exchange) & 
                        (chunk['trading_pair'] == pair)
                    ].copy()
                    
                    if len(pair_data) == 0:
                        continue
                    
                    # Extract LOB data
                    lob_records = []
                    for _, row in pair_data.iterrows():
                        lob_dict = extract_lob_data(row['data'], LOB_LEVELS)
                        if lob_dict:
                            record = {
                                'timestamp': row['timestamp'],
                                'exchange': exchange,
                                'trading_pair': pair,
                                **lob_dict
                            }
                            lob_records.append(record)
                    
                    if lob_records:
                        # Convert to DataFrame
                        pair_df = pd.DataFrame(lob_records)
                        pair_df['timestamp'] = pd.to_datetime(pair_df['timestamp'])
                        pair_df = pair_df.sort_values('timestamp')
                        
                        # Detect gaps
                        gaps = detect_gaps(pair_df, MAX_GAP_MINUTES)
                        
                        # Split into sessions
                        sessions = split_into_sessions(pair_df, gaps, MIN_SESSION_HOURS)
                        
                        # Save each session
                        for session_idx, session_df in enumerate(sessions):
                            if len(session_df) > 0:
                                session_file = os.path.join(
                                    date_dir, 
                                    f"{exchange}_{pair}_{date_str}_session_{session_idx}.parquet"
                                )
                                session_df.to_parquet(session_file, compression='snappy', index=False)
                                
                                logger.info(f"    Saved {exchange} {pair} session {session_idx}: {len(session_df)} records")
            
            processed_records += len(chunk)
            
            # Memory cleanup
            del chunk
            gc.collect()
        
        conn.close()
        
        logger.info(f"  Completed {date_str}: {processed_records} records processed")
        
        # Save daily summary
        summary = {
            'date': date_str,
            'total_records': processed_records,
            'processing_time': datetime.now().isoformat(),
            'exchanges': TARGET_EXCHANGES,
            'trading_pairs': TARGET_PAIRS
        }
        
        summary_file = os.path.join(date_dir, f"{date_str}_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        return True
        
    except Exception as e:
        logger.error(f"Error processing {date_str}: {e}")
        return False

def create_resample_data(input_dir: str, output_dir: str, frequency: str = '5s') -> bool:
    """Resample processed data to consistent frequency."""
    logger.info(f"Resampling data to {frequency} frequency...")
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        # Process each date directory
        for date_dir in os.listdir(input_dir):
            date_path = os.path.join(input_dir, date_dir)
            if not os.path.isdir(date_path):
                continue
            
            logger.info(f"  Resampling {date_dir}...")
            
            # Find all session files
            session_files = [f for f in os.listdir(date_path) if f.endswith('.parquet')]
            
            for session_file in session_files:
                session_path = os.path.join(date_path, session_file)
                
                try:
                    # Load session data
                    df = pd.read_parquet(session_path)
                    
                    if len(df) == 0:
                        continue
                    
                    # Set timestamp as index
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df = df.set_index('timestamp')
                    
                    # Resample to target frequency
                    df_resampled = df.resample(frequency).last()
                    
                    # Forward fill missing values (up to 6 periods = 30 seconds)
                    df_resampled = df_resampled.fillna(method='ffill', limit=6)
                    
                    # Drop rows with remaining NaN values
                    df_resampled = df_resampled.dropna()
                    
                    if len(df_resampled) > 0:
                        # Save resampled data
                        output_file = os.path.join(output_dir, f"{session_file.replace('.parquet', f'_resampled_{frequency}.parquet')}")
                        df_resampled.to_parquet(output_file, compression='snappy')
                        
                        logger.info(f"    Resampled {session_file}: {len(df)} â†’ {len(df_resampled)} records")
                
                except Exception as e:
                    logger.error(f"Error resampling {session_file}: {e}")
                    continue
        
        return True
        
    except Exception as e:
        logger.error(f"Error in resampling: {e}")
        return False

def main():
    """Main extraction pipeline."""
    logger.info("Starting enhanced LOB data extraction...")
    logger.info(f"Target: {len(HIGH_QUALITY_DAYS)} high-quality days")
    logger.info(f"Output directory: {OUTPUT_DIR}")
    
    # Create output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Process high-quality days
    successful_days = 0
    failed_days = []
    
    for date_str in HIGH_QUALITY_DAYS:
        logger.info(f"Processing day {successful_days + 1}/{len(HIGH_QUALITY_DAYS)}: {date_str}")
        
        if process_single_day(date_str):
            successful_days += 1
        else:
            failed_days.append(date_str)
    
    logger.info(f"Extraction completed: {successful_days}/{len(HIGH_QUALITY_DAYS)} days successful")
    
    if failed_days:
        logger.warning(f"Failed days: {failed_days}")
    
    # Create resampled data
    resampled_dir = os.path.join(OUTPUT_DIR, "resampled_5s")
    if create_resample_data(OUTPUT_DIR, resampled_dir, '5s'):
        logger.info("Resampling completed successfully")
    else:
        logger.error("Resampling failed")
    
    # Create extraction summary
    summary = {
        'extraction_date': datetime.now().isoformat(),
        'target_days': len(HIGH_QUALITY_DAYS),
        'successful_days': successful_days,
        'failed_days': failed_days,
        'exchanges': TARGET_EXCHANGES,
        'trading_pairs': TARGET_PAIRS,
        'lob_levels': LOB_LEVELS,
        'max_gap_minutes': MAX_GAP_MINUTES,
        'min_session_hours': MIN_SESSION_HOURS
    }
    
    summary_file = os.path.join(OUTPUT_DIR, "extraction_summary.json")
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    logger.info("Enhanced LOB data extraction completed!")
    logger.info(f"Summary saved to: {summary_file}")

if __name__ == "__main__":
    main() 